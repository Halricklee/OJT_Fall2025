[
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "AWS Open Source Blog AWS joins the DocumentDB project to build interoperable, open source document database technology by Rashim Gupta on 24 AUG 2025 in Amazon DocumentDB, Announcements, Database, Open Source | Permalink\nAt AWS, we design cloud services that give customers the freedom to choose technology that best suits their needs. Our commitment to interoperability with open standards and open source technologies is a key reason customers choose AWS. This is one of the reasons why we launched Amazon DocumentDB (with MongoDB compatibility) in 2019.\nAmazon DocumentDB is a serverless, fully managed, MongoDB API-compatible document database service. Amazon DocumentDB serves tens of thousands of customers globally across all industries. For example, United Airlines uses Amazon DocumentDB to modernize their ticket ordering workflow. Capital One uses Amazon DocumentDB for its credit decisioning application. FINRA leveraged Amazon DocumentDB to revamp how regulatory filings are collected on behalf of their customers to support its mission.\nToday, AWS joins the open source DocumentDB project under the stewardship of the Linux Foundation, reiterating our commitment to choice and interoperability for customers. Microsoft launched the DocumentDB project in January 2025. Microsoft has moved the project to The Linux Foundation, where it will be independently led and governed. The project aims to provide the developer community with a PostgreSQL-based document database, with complete visibility into the architecture and implementation of the engine. Because the project is open source under the permissive MIT license, developers and organizations alike can migrate existing applications with little to no change or build new applications. AWS has joined the technical steering committee of the Linux Foundation project and will contribute to the further advancement of this important technology.\nIn this post, we present three reasons why AWS joined the DocumentDB open source project.\n1. MongoDB API compatibility First, DocumentDB has a goal to provide 100% compatibility with the MongoDB API. The MongoDB API is the most popular document database API, and we want to see it continue to succeed. By joining this project, we are helping customers access the same compatibility, performance, and functionality, no matter where they run their applications – in AWS, other clouds, on-premises, or locally on their desktops.\n2. Feature innovations Second, open source accelerates innovation. With multiple database vendors and cloud providers, including Microsoft and Yugabyte, joining the project, we expect the community to not just close the gap with MongoDB API compatibility but also improve performance, features, and developer experience. Each organization brings their expertise to the table. With multiple companies and contributors supporting this project, customers will be able to leverage the advancements made in the project, while still being compatible with the MongoDB API.\n3. Built on PostgreSQL Third, DocumentDB is built on PostgreSQL, which has become the preferred relational database for many enterprise developers and startups, with its reputation for reliability, features, and extensibility that’s backed with nearly 35 years of active development. AWS is recognized as a major contributor to the PostgreSQL open source community, providing contributions to the core database software, extensions, drivers, and project governance. AWS has contributed to PostgreSQL features for high availability, major version upgrades, improved performance for queries and maintenance operations, JDBC driver, extensions like pgvector, and community operations. For example, Trusted Language Extensions (pg_tle) for building extensions on restricted filesystems, and pgactive for added resiliency and flexibility for moving data between two or more active databases. Now, by joining the DocumentDB project, we will bring together our expertise on Amazon DocumentDB and PostgreSQL into improvements on the open source project that will benefit customers.\nLooking ahead Since its inception, AWS has been the best place for customers to build and run open source software in the cloud. AWS is proud to support open source projects, foundations, and partners. We are committed to bringing the value of open source to our customers, and the operational excellence of AWS to open source communities.\nAs part of our open source commitment, AWS has a long history of contributing to open source projects. For example, we have large number of contributions to Lucene, Valkey, containerd, PostgreSQL, Rust, and OpenSearch. We’ve learned a lot from those experiences, including the importance of getting involved and contributing upstream to the open source projects that we build on and that our customers depend on. AWS remains committed to DocumentDB for the long term because we firmly believe in the power of open source to increase innovation. Similarly to Valkey and OpenSearch, we will continue to bring the innovations we have built in Amazon DocumentDB to the open source DocumentDB project so that customers can continue to take advantage of those capabilities, no matter where they choose to run their software.\nWhile the DocumentDB project, stewarded by Linux Foundation, has a similar name to Amazon DocumentDB, they use different software under the hood. Amazon DocumentDB is a MongoDB API-compatible document database that is built by AWS. The project, stewarded by Linux Foundation, on the other hand, while also being MongoDB compatible, uses an open source engine that is built as an extension on PostgreSQL. This is a different engine than the one used in Amazon DocumentDB. AWS will continue to invest in both Amazon DocumentDB and open source DocumentDB akin to how we invest in Amazon OpenSearch Service and OpenSearch. Moving forward, we will start contributing Amazon DocumentDB innovations to the open source project, and adopt features and capabilities from the open source DocumentDB engine to our managed Amazon DocumentDB service over time. We will announce these changes in our What’s New posts over the coming months.\n“It’s great that Microsoft, AWS, and others are joining forces to work on DocumentDB, an open source implementation of a MongoDB-compatible API on top of PostgreSQL. Microsoft and AWS already work together to enhance Postgres, so it is logical they would use the high-quality Postgres source code and leverage its extensibility to meet the need for an open source document database,” said Bruce Momjian, founding member of the PostgreSQL core development team. “The idea of using Postgres in this way has been around for a long time so I am glad it is now getting serious traction. DocumentDB should be an interesting alternative to users wanting an open source implementation, and for database users who just want a simpler interface to PostgreSQL.”\n“DocumentDB fills a critical gap in the document database ecosystem, attracting contributors, users and champions. What’s even more exciting is it provides an open standard for document based applications, like what SQL did for relational databases,” said Jim Zemlin, executive director of the Linux Foundation. “By joining the Linux Foundation, DocumentDB is securing its open source future and helping chart a new path for NoSQL database standards and community-driven innovation.”\nWe are excited to contribute to DocumentDB project as one of many stakeholders. Please join us on GitHub to continue open source development on DocumentDB. You can also visit our website here to learn more and get involved.\nRashim Gupta\nRashim Gupta is a Senior Manager, Product Management at AWS. Over the last seven years, Rashim has led product for multiple database services at AWS. He currently leads the PM teams for Amazon DocumentDB, Amazon Timestream, and Amazon Neptune. Prior to his current role, he led the launch for Valkey in Amazon ElastiCache and Amazon MemoryDB. He has also worked on storage and compute services in AWS, analytics in Microsoft Azure, and developer experience at Meta.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Lê Quang Huy\nPhone Number: 0977508831\nEmail: leequanghuy2004@gmail.com\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS092025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 09/09/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "On this page, you will find my comprehensive 12-week AWS learning journey worklog. This program was designed to build foundational to advanced AWS cloud computing skills through hands-on practice with First Cloud Journey (FCJ) workshops.\nHow did I complete it? I followed a structured learning path from September 9, 2025, focusing on practical labs and real-world scenarios. Each week consisted of 5 days (Monday to Friday) of intensive learning, with 10-15 hours dedicated to hands-on practice, documentation reading, and project implementation.\nProgram Duration: 12 weeks (3 months)\nLearning Approach:\n60% hands-on practice with AWS services 30% reading AWS documentation and FCJ workshop materials 10% completing exercises and reviewing best practices What did I accomplish?\nThroughout this 12-week program, I progressively built expertise across 25+ AWS services, starting from account setup to deploying production-ready, highly available applications. Below is the weekly breakdown:\nWeek 1: AWS Fundamentals - Account Setup, Billing, Support, and IAM\nWeek 2: AWS Networking - VPC, Subnets, Security Groups, and AWS CLI\nWeek 3: Compute Services - EC2, IAM Roles, and AWS Cloud9\nWeek 4: Storage and Simplified Computing - S3 Static Hosting, Lightsail, and Containers\nWeek 5: Database Fundamentals - Amazon RDS and DynamoDB\nWeek 6: Database Optimization - ElastiCache and 3-Tier Architecture\nWeek 7: Scaling and Monitoring - Auto Scaling and CloudWatch\nWeek 8: Global Distribution - Route 53, CloudFront, and Lambda@Edge\nWeek 9: Windows on AWS - Windows Server EC2 and Managed Microsoft AD\nWeek 10: Hybrid Directory Services - AD Integration and Enterprise Authentication\nWeek 11: High Availability - Load Balancing, Multi-Tier Apps, and Disaster Recovery\nWeek 12: Capstone Project - Comprehensive Application Deployment and Certification Prep\nKey Achievements:\nMastered 15+ AWS services across compute, storage, database, networking, and security Built production-ready architectures with high availability and fault tolerance Implemented cost optimization and monitoring strategies Gained hands-on experience with Windows workloads and enterprise directory services Completed comprehensive capstone project integrating all learned services Next Steps:\nPursue AWS Certified Solutions Architect - Associate certification Continue exploring advanced AWS services Contribute to AWS community and open-source projects "
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/4-eventparticipated/4.1-event1/",
	"title": "Kick-off AWS FCJ Workforce",
	"tags": [],
	"description": "",
	"content": "Summary Report: “Kick-off AWS FCJ Workforce -FPTU OJT FALL 2025” Event Objectives Build a new generation of high-quality AWS Builders for Vietnam. Equip students with hands-on skills in Cloud, DevOps, AI/ML, Security, and Data \u0026amp; Analytics. Connect students with the AWS Study Group community of 47,000+ members and AWS partner companies. Deliver strategic career orientation covering specialized fields like Cloud Computing, DevOps, and GenAI. Speakers Nguyễn Gia Hưng – Head of Solutions Architect, AWS Vietnam Đỗ Huy Thắng – DevOps Lead, VNG Danh Hoàng Hiếu Nghị – GenAI Engineer, Renova Bùi Hồ Linh Nhi – AI Engineer, SoftwareOne Phạm Nguyễn Hải Anh – Cloud Engineer, G-Asia Pacific Nguyễn Đồng Thanh Hiệp – Principal Cloud Engineer, G-Asia Pacific Introduce AWS program Sharing what we have to do when we join AWS program Speak about devOps, advantage and the other services we are about to learn Introduce exccelent member in the AWS and things they have don AWS First Cloud Journey \u0026amp; Future Direction The AWS First Cloud Journey marks the beginning of a transformative path toward building cloud expertise and innovation. It empowers learners and organizations to explore the core foundations of AWS services — from cloud computing, networking, and storage to advanced topics like DevOps, AI/ML, and data analytics. This journey not only provides practical, hands-on experience but also cultivates a mindset of agility, scalability, and continuous improvement. Looking toward the future direction, AWS aims to foster a new generation of cloud professionals who can drive digital transformation, implement sustainable cloud solutions, and harness cutting-edge technologies to shape the future of Vietnam’s digital economy. DevOps \u0026amp; Future Career Bridge between Development and Operations: Connects software development and IT operations to enhance collaboration, speed, and reliability. Culture of Continuous Improvement: Focuses on automation, testing, monitoring, and feedback for faster and higher-quality delivery. High Demand for DevOps Professionals: Cloud adoption drives strong demand for DevOps experts across all industries. Key Future Skills: Cloud platforms (AWS, Azure, GCP) → CI/CD pipelines → containerization (Docker, Kubernetes) → infrastructure as code (Terraform, CloudFormation). Career Growth Opportunities: Roles like DevOps Engineer, Cloud Architect, SRE, and Platform Engineer will continue expanding globally. Mindset for Success: Agility, collaboration, problem-solving, and continuous learning are essential for thriving in the DevOps era. From First Cloud Journey to GenAI Engineer First Cloud Journey: The starting point to explore cloud computing, mastering the fundamentals of AWS services and architecture. Building Cloud Foundations: Learn key domains such as networking, storage, security, and compute to understand how modern cloud systems operate. DevOps Integration: Adopt automation, CI/CD pipelines, and Infrastructure as Code to enable faster and more reliable deployments. Data \u0026amp; Analytics Skills: Gain the ability to collect, process, and visualize data for informed business decisions. AI/ML Advancement: Move from cloud operations to applying machine learning and artificial intelligence using AWS AI services. GenAI Engineer Path: Leverage cloud, DevOps, and AI expertise to design, deploy, and optimize generative AI applications for real-world innovation. She in Tech \u0026amp; The Journey with First Cloud Journey Finding and create a team that follow you entire carrier: Join in Hackathon and create many projects. Memories: Create many memories with your team and become better A Day in the Life of a Cloud Engineer Morning Check-ins: Review system dashboards, monitor cloud resources, and ensure all services are running smoothly. Infrastructure Management: Use Infrastructure as Code (IaC) tools like Terraform or CloudFormation to deploy and manage environments. Security \u0026amp; Compliance: Apply IAM policies, monitor access logs, and ensure systems meet security best practices. Automation \u0026amp; DevOps Tasks: Build CI/CD pipelines, automate deployments, and optimize performance for scalability. Collaboration \u0026amp; Problem-Solving: Work with developers, DevOps, and data teams to troubleshoot and improve cloud solutions. Learning \u0026amp; Innovation: Stay updated with the latest AWS services, explore AI/ML integrations, and continuously improve cloud architecture skills. Event Experience Attending the “Kick-off AWS FCJ Workforce -FPTU OJT FALL 2025” workshop was extremely valuable, giving me a comprehensive view of people have been studied and exprienced. We also know the others services in AWS and witnessed many project that was created by many exprienced people and could hear a story of the team AWS their journey to become the best developer.\nLessons learned Know that we are going to do in AWS company. Make Friends and you could learn from your mentor. Got motivation to working in AWS. Overall, the event just about introduce about the company , people and other services . I also got motivation to move forward and learn as best as i can of AWS services.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Goals (Sep 8 – Sep 14, 2025) Get familiar with AWS console and main services. Set up an AWS Free Tier account with proper security measures, including MFA. Learn to monitor spending using AWS Budgets and billing alerts. Gain hands-on experience with IAM: users, groups, and permission policies. Activities \u0026amp; Tasks Day Description Start End References Monday, Sep 8 Created AWS Free Tier account, enabled MFA for root user, and configured billing alerts. 08/09/2025 08/09/2025 AWS Docs Tuesday, Sep 9 Explored AWS Budgets: created budget, set email alerts, and checked Cost Explorer. 09/09/2025 09/09/2025 AWS Budgets Guide Wednesday, Sep 10 Reviewed AWS Support plans (Basic, Developer, Business) and learned about response times. 10/09/2025 10/09/2025 AWS Support Thursday, Sep 11 Studied IAM fundamentals, created users and groups, and applied sample policies. 11/09/2025 11/09/2025 IAM Docs Friday, Sep 12 Hands-on practice: set MFA for IAM users, tested permissions, created \u0026amp; deleted an S3 bucket to understand Free Tier limits. 12/09/2025 12/09/2025 S3 Docs Achievements Successfully created and secured AWS Free Tier account with MFA. Able to set up budgets and alerts to track spending. Understood different AWS Support plans and their response times. Practiced IAM setup: users, groups, and policy management. Gained experience with basic S3 operations while staying within Free Tier limits. Reflection By the end of Week 1, I am confident navigating AWS Management Console, managing account security and costs, and implementing IAM for safe access. Hands-on S3 practice helped me understand Free Tier restrictions practically.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/5-workshop/5.4-api-gateway-lambda/5.4.1-rest-api/",
	"title": "API Gateway REST",
	"tags": [],
	"description": "",
	"content": "REST API Gateway We will create a REST API to connect the frontend to backend Lambda functions.\nRequired routes:\nPOST /score GET /leaderboard GET /leaderboard/global POST /progress POST /unlock POST /task/complete POST /money/add POST /shop/buy POST /avatar/presign POST /avatar/update POST /avatar/process ← calls the Lambda container for avatar processing Configuration:\nEnable CORS for all routes. Create a JWT Authorizer pointing to Cognito. Attach each route to the correct Lambda function. "
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/5-workshop/5.3-dynamodb-s3/5.3.1-dynamodb/",
	"title": "Create DynamoDB",
	"tags": [],
	"description": "",
	"content": "DynamoDB – Main Database In the game project, we use DynamoDB to store player profiles, progress, and scores.\nCreate 3 tables:\nUserProfiles Partition Key (PK): userId UserProgress Partition Key (PK): userId Scores Partition Key (PK): gameArea Sort Key (SK): score Enable DynamoDB Stream\nStream type: NEW IMAGE DevOps provides the Stream ARN to the backend so Lambda can consume events. "
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/5-workshop/5.1-workshop-overview/",
	"title": "Workshop Overview",
	"tags": [],
	"description": "",
	"content": "Workshop Objective In this workshop, you will learn how to deploy a backend system for a game, including:\nCognito: login system with Username/Password and OAuth (Google). DynamoDB: storing player profiles, progress, and scores. S3: storing player avatars, only allowing uploads via pre-signed URL. Lambda: backend APIs using standard ZIP Lambdas and Container Lambda (OpenCV). API Gateway REST \u0026amp; WebSocket: serving REST routes and realtime leaderboard. CI/CD (CodePipeline + CodeBuild): automatically build container and deploy Lambda. IAM Roles: permissions for Lambda and CodeBuild. CloudWatch: basic logging, billing alarm, and error alarm. You will deploy each AWS service step by step, test using CLI or console, and finally collect all necessary information to hand over to FE and BE teams.\nLab Sections Cognito – Login System DynamoDB – Main Database S3 – Store Player Avatars ECR – Avatar Processing Container Lambda – Backend APIs API Gateway REST API Gateway WebSocket CI/CD – Backend IAM Roles Logging \u0026amp; Monitoring WAF (Optional) Prerequisites AWS account with sufficient IAM permissions to create and configure the services above. Use ap-southeast-2 region for this workshop. "
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/4-eventparticipated/4.2-event2/",
	"title": "AI-Driven Development Life Cycle",
	"tags": [],
	"description": "",
	"content": "Summary Report: ““AI-Driven Development Life Cycle: Reimagining Software Engineering (AWS Cloud Mastery Series #1) Event Objectives Present a market update on the AI/ML adoption and current trends within the local region. Provide hands-on training for developing end-to-end ML models using Amazon SageMaker. Conduct a technical deep dive into Generative AI capabilities on Amazon Bedrock, including its Foundation Models, Agents, and Guardrails. Provide essential skills training in Prompt Engineering and the deployment of Retrieval-Augmented Generation (RAG) systems. Speakers AWS Experts Team Key Highlights Program Introduction \u0026amp; Networking Market Insight: Shared an update on the current state of Artificial Intelligence and Machine Learning adoption across the Vietnam market. AWS AI/ML Services: The SageMaker Platform Unified ML Workflow: Detailed the complete machine learning lifecycle managed on Amazon SageMaker, covering data preparation, labeling, model training, tuning, and integration with MLOps for automated deployment. Studio Walkthrough: A live demonstration provided a clear view of the SageMaker Studio interface and its professional data science features. Generative AI Deep Dive with Amazon Bedrock Model Selection: Offered criteria and comparisons for selecting the optimal Foundation Models, such as Claude, Llama, and Titan, based on specific use case requirements. Prompt Optimization: Covered advanced Prompt Engineering techniques, including Chain-of-Thought reasoning and Few-shot learning. RAG Implementation: Demonstrated the \u0026ldquo;Retrieval -\u0026gt; Augmentation -\u0026gt; Generation\u0026rdquo; architecture, focusing on how to integrate internal Knowledge Bases to significantly improve AI response accuracy. Advanced Controls: Introduced Bedrock Agents for managing complex, multi-step tasks and Guardrails for ensuring content safety and regulatory compliance. Live Build: Successfully built a functional GenAI Chatbot prototype using Amazon Bedrock during the session. Key Takeaways Platform Strategy SageMaker\u0026rsquo;s Role: The ideal, robust platform for managing traditional, iterative Machine Learning development cycles with governance. Bedrock\u0026rsquo;s Advantage: Provides a fast, simplified API-based route to deploying Generative AI applications without the overhead of managing complex underlying infrastructure. Advanced Implementation Beyond Simple Chat: RAG and Agents are critical technologies that enable GenAI to move past basic interactions and solve complex, real-world business challenges and workflows. Compliance and Safety: Guardrails are an obligatory component for any enterprise deployment, ensuring the AI operates within defined ethical, safety, and regulatory boundaries. Applying to Work Standardize MLOps: Apply the learned SageMaker standards to automate the model lifecycle, tracking, and governance in ongoing projects. Develop RAG PoC: Conduct experiments to integrate proprietary company documents into a Bedrock Knowledge Base to build a context-aware information retrieval assistant. Improve AI Quality: Implement Chain-of-Thought prompting techniques to enhance the logical consistency and output quality of existing AI applications. Strategic Selection: Utilize the learned criteria to make informed decisions when selecting Foundation Models, balancing performance requirements against cost considerations. Event Experience This workshop provided a highly valuable balance between fundamental Machine Learning principles and contemporary Generative AI practices.\nPractical Application The SageMaker Studio walkthrough offered a clear visualization of a professional ML workflow. The Bedrock Chatbot demonstration was a highlight, showcasing the rapid development possible for powerful GenAI applications. Market Relevance The analysis of the local AI market helped frame my technical work within the broader industry trends and identified future opportunities. The event help me understand more about AI-Driven Development Life Cycle:\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "AWS Training and Certification Blog Unlock the power of Amazon OpenSearch Service: Your learning guide for search, analytics, and generative AI solutions by Manika Dhingra on 22 AUG 2025 in Amazon OpenSearch Service, Analytics, AWS Training and Certification, Best Practices | Permalink\nWhether you’re a developer, data analyst, DevOps engineer, or IT leader, understanding how to unlock the full potential of your search and analytics workloads is critical for success in cloud computing. Amazon OpenSearch Service has evolved into a versatile powerhouse organizations use to build sophisticated search applications, derive meaningful insights through analytics, and now use generative AI capabilities. This comprehensive guide can help you navigate through the learning resources available for each use case so you build the right skills for your specific needs.\nAWS Skill Builder trainings AWS Skill Builder offers comprehensive self-paced digital courses to build your OpenSearch Service expertise:\nGetting Started with Amazon OpenSearch Service – This 1-hour fundamental course covers essential concepts, benefits, and use cases of OpenSearch Service, including search queries, log analytics, and dashboard creation for data visualization.\nEmpower Search with AI using Amazon OpenSearch Service – This is a 2-hour advanced course that covers AI-driven search capabilities using OpenSearch Service. The course explores search engine fundamentals, semantic search, vector search implementation, neural sparse search, and vector database sizing principles, with a focus on balancing computational efficiency and performance optimization.\nMigrating from Elasticsearch to Amazon OpenSearch Service – This detailed 3-hour course outlines the six-stage migration journey from Elasticsearch to OpenSearch Service. It covers migration planning, proof-of-concept development, implementation patterns, and operational best practices.\nImplement Lexical Search Using Amazon OpenSearch Service – This 2-hour course focuses on OpenSearch Service capabilities such as data ingestion options, client communication, mapping functionality, aggregation types, performance tuning options, and performing lexical and advanced search queries.\nAmazon OpenSearch Service – Building Observability Solutions – This is a 1-hour course that teaches how to analyze telemetry data using OpenSearch Service observability features, including domain creation, dashboard building, and analysis of logs, traces, and metrics.\nOptimizing Foundation Models – This 1-hour course covers OpenSearch Service vector database capabilities for RAG implementations, teaching how to store and manage embeddings for AI applications.\nBuilding Retrieval Augmented Generation (RAG) workflows with Amazon OpenSearch Service – This 1-hour course explains RAG techniques and their role in enhancing large language model (LLM) performance. The course covers LLM workflows, knowledge base creation, OpenSearch search methods, and the integration of Amazon OpenSearch Serverless with Amazon Bedrock Knowledge Bases for embeddings and information retrieval.\nHands-on learning with AWS Workshops For those seeking practical experience with OpenSearch Service, AWS Workshops provide guided, hands-on learning through step-by-step instructions to build search, analytics, and AI solutions using the AWS Management Console.\nSearch and AI: Semantic and vector search with Amazon OpenSearch Service – Demonstrates using OpenSearch Service vector database capabilities for semantic search, neural search, hybrid search, and conversational search.\nUse OpenSearch Service as a vector store for gen AI applications – Explores integrating OpenSearch Service with LLMs for generative AI applications, focusing on vector search and retrieval for improved accuracy.\nWhen life hands you data, grab OpenSearch – Introduces working with OpenSearch for data ingestion, search, aggregation, and analysis, with a focus on application search and log analytics.\nGetting started with Amazon OpenSearch Serverless – Provides hands-on experience with Amazon OpenSearch Serverless, covering setup, query types, visualizations, and security configurations.\nLog analytics and observability: Microservice observability with Amazon OpenSearch Service – Teaches how to use open source tools and OpenSearch Service for observability in distributed applications, covering logs, traces, and metrics.\nSIEM on Amazon OpenSearch Service Workshop – Guides building a security information and event management (SIEM) system on OpenSearch Service for log ingestion, analysis, and dashboarding.\nAWS Certifications AWS Certifications validate your technical expertise in specific domains of cloud computing. For professionals working with OpenSearch, several certifications demonstrate proficiency in search, analytics, and data solutions:\nPrimary certifications: AWS Certified Data Engineer – Associate – Validates expertise in designing data models, managing data lifecycles, and ensuring data quality, essential for OpenSearch Service implementations.\nAWS Certified Solutions Architect – Professional – Demonstrates the ability to design distributed systems and complex architectures incorporating OpenSearch Service.\nSupporting certifications: AWS Certified Machine Learning – Specialty – For those using OpenSearch Service in machine learning (ML) pipelines or for advanced search capabilities, this certification validates expertise in building, training, tuning, and deploying ML models on AWS.\nAWS Certified DevOps Engineer – Professional – Validates skills in operating and scaling OpenSearch Service clusters and implementing observability solutions.\nAdditional resources To learn more, refer to the following resources.\nCommunity engagement: OpenSearch Community Calls AWS User Groups Online resources: AWS blog posts on Amazon OpenSearch Service AWS Solutions Library Staying current The OpenSearch ecosystem is rapidly evolving, especially in areas such as vector search and generative AI. Regular engagement with these learning resources keeps your skills current and valuable. AWS frequently updates training content to reflect new features and best practices.\nReady to begin your OpenSearch Service journey? Visit AWS Skill Builder to access these resources and start building your expertise today.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Goals (Sep 15 – Sep 21, 2025) Explore EC2 instances: types, pricing, and launch process. Understand VPC basics and networking configurations. Practice connecting to EC2 instances via SSH. Learn about S3 storage classes and lifecycle management. Activities \u0026amp; Tasks Day Description Start End References Monday, Sep 15 Learned EC2 instance types, pricing options, and key use cases. 15/09/2025 15/09/2025 AWS EC2 Docs Tuesday, Sep 16 Launched first EC2 instance, configured security group rules, and assigned key pair. 16/09/2025 16/09/2025 EC2 Launch Guide Wednesday, Sep 17 Connected to EC2 instance via SSH, performed basic OS commands, and verified connectivity. 17/09/2025 17/09/2025 SSH Connection Docs Thursday, Sep 18 Studied VPC: subnets, route tables, internet gateways, and network ACLs. 18/09/2025 18/09/2025 VPC Basics Friday, Sep 19 Explored S3 storage classes (Standard, IA, Glacier), uploaded files, and applied lifecycle policies. 19/09/2025 19/09/2025 S3 Lifecycle Docs Achievements Understood EC2 instance types and their proper use cases. Successfully launched EC2 instance and connected via SSH. Gained hands-on experience with VPC configurations. Learned different S3 storage classes and applied lifecycle policies to manage objects effectively. Reflection Week 2 deepened my understanding of AWS compute and networking services. Working with EC2 and VPC gave practical insights into cloud infrastructure, while S3 lifecycle management showed how to optimize storage costs.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/5-workshop/5.3-dynamodb-s3/5.3.2-s3/",
	"title": "Create S3",
	"tags": [],
	"description": "",
	"content": "S3 – Store Player Avatars Create an S3 bucket:\nBucket name: game-avatars Enable CORS: allow PUT, POST, GET from all origins *. Note: Only allow uploads via presigned URLs generated by the backend.\nSummary You have created 3 DynamoDB tables and an S3 bucket. These resources will be used by the backend to store player data and avatars.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM Permissions Add the following IAM policy to your user account to deploy and manage the game backend project:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;GameProjectPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:*\u0026#34;, \u0026#34;apigateway:*\u0026#34;, \u0026#34;cognito-idp:*\u0026#34;, \u0026#34;dynamodb:*\u0026#34;, \u0026#34;s3:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;codebuild:*\u0026#34;, \u0026#34;codepipeline:*\u0026#34;, \u0026#34;logs:*\u0026#34;, \u0026#34;sns:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } "
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Serverless Multiplayer Game Backend A Scalable AWS Solution for Real-Time Gaming \u0026amp; AI Avatar Processing\n1. Executive Summary This proposal outlines the development of a fully serverless backend infrastructure for a multiplayer Unity game.\nThe system is divided into three main responsibilities: DevOps, Backend, and Unity Frontend.\nAWS services will be used to provide:\nUser Authentication → Amazon Cognito Gameplay \u0026amp; Business Logic → AWS Lambda Real-time Leaderboards → API Gateway WebSocket + DynamoDB Streams AI Avatar Generation → Lambda Container (OpenCV/MediaPipe) This architecture ensures scalability, fault tolerance, CI/CD automation, and supports seamless deployment for WebGL via itch.io or CloudFront.\n2. Problem Statement What’s the Problem? Multiplayer games require real-time backend capability, identity management, and persistent state storage.\nTraditional server-based systems:\nare expensive to run long-term scale poorly under heavy traffic spikes increase maintenance overhead Additionally, this project requires AI avatar transformation, which demands high compute power.\nThe Solution A scalable serverless architecture using AWS services:\nAuthentication: Amazon Cognito (User Pools + Hosted UI) Logic: AWS Lambda (Zip + ECR container images) Storage: S3 for avatars + DynamoDB for player metadata Realtime: WebSocket API + DynamoDB Streams Benefits \u0026amp; ROI Highly Cost Efficient → Pay-per-execution, no idle servers Auto Scaling → Handles traffic bursts without manual scaling CI/CD Ready → Quick feature rollout through CodePipeline/CodeBuild 3. Solution Architecture The system uses an event-driven microservice approach.\nUnity communicates with backend via REST APIs and WebSocket live streams, while avatar processing runs via container-based Lambda.\nAWS Services Used Category Services Identity Amazon Cognito API API Gateway REST + WebSocket Compute AWS Lambda (Zip + Container) Database DynamoDB + Streams Storage S3 Image Processing ECR Container Lambda CI/CD CodePipeline, CodeBuild Component Design Frontend Unity WebGL build deployed on itch.io / CloudFront.\nData Flow User Login → Cognito Issues Token Unity → REST API (Lambda → DynamoDB) Avatar Upload → Presigned URL → S3 AI Avatar Processor → Lambda Container (OpenCV/MediaPipe) Score Updates → DynamoDB Stream → WebSocket Broadcast Live 4. Technical Implementation Implementation Phases Infrastructure Setup (DevOps)\nCognito, DynamoDB, S3, API Gateway\nBackend Skeleton (BE)\nLambda endpoints + Postman scripts + API routing\nFrontend Login (FE)\nAuthManager + Cognito integration\nIntegration Wiring (DevOps)\nREST + WebSocket + DynamoDB Streams wiring\nGameplay API Integration (FE)\nDataManager for score, money, progress, tasks\nEnd-to-End Testing\nLogin → Game API → Leaderboard → Avatar\nDeployment\nUnity WebGL build + Allowed Redirect URLs\nTech Stack Requirements Role Stack Frontend Unity (C#), AwsConfig, AuthManager, DataManager, RealtimeManager Backend Lambda NodeJS/Python, Docker Container (OpenCV/MediaPipe) DevOps IAM, API Gateway, DynamoDB Streams, CodePipeline/Build 5. Timeline \u0026amp; Milestones Phase Duration Deliverables Foundation Days 1–3 Cognito, DynamoDB, S3, API Gateway Logic Development Days 3–8 Lambda API + Avatar Container + Unity Login Integration Days 8–12 Streams wired, APIs connected Testing \u0026amp; Launch Days 13–15 WebGL live deployment + Leaderboard \u0026amp; Avatar tests 6. Budget Estimation (Based on AWS Pricing Calculator)\nResource Cost Lambda Free Tier (low execution) DynamoDB Free Tier (25GB) S3 Storage $0.023/GB CloudWatch Logs ~$0.5 - $1/month ECR Storage ~$0.10/GB Estimated Total Cost \u0026lt; $5/month during development.\n7. Risk Assessment Risk Matrix Risk Impact Probability Integration complexity High Medium Latency during peak loads Medium Low Cost overrun Low Low Mitigation Strategy Mock APIs using Postman during FE development Placeholder logic for shop \u0026amp; leaderboard CloudWatch alarm triggers on error spikes (\u0026gt;10/min) 8. Expected Outcomes Technical Outcomes Fully serverless backend infrastructure Secure identity \u0026amp; data persistence Real-time leaderboard support Automated avatar processing pipeline Long-Term Value Reusable architecture for any future game titles Auto-scales without provisioning servers Extremely low operational cost "
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/5-workshop/5.4-api-gateway-lambda/5.4.2-websocket-api/",
	"title": "Websocket API",
	"tags": [],
	"description": "",
	"content": "WebSocket API Gateway WebSocket API is used for real-time leaderboard.\nRoutes:\n$connect → Lambda handler saves connectionId $disconnect → Lambda handler removes connectionId broadcast → Lambda handler sends leaderboard Configuration:\nLambda functions can read connectionIds from DynamoDB. "
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/4-eventparticipated/4.3-event3/",
	"title": "​DevOps with AWS",
	"tags": [],
	"description": "",
	"content": "Summary Report: ​DevOps on AWS (AWS Cloud Mastery Series #2) Event Objectives Explore more about DevOps services and DevOps\u0026rsquo;s mindset Know more about infrastructe as code Learn more container Service on AWS Learn more about monitoring and observability Speakers -Bao Huynh – AWS Community Builders -Thinh Nguyen – AWS Community Builders -Vi Tran – AWS Community Builders -Long Huynh – AWS Community Builders -Quy Pham – AWS Community Builders -Nghiem le – AWS Community Builders\nKey Highlights AWS CodeCommit Secure, scalable Git-based repository service on AWS Supports GitFlow, Trunk-Based, and feature-branch workflows Ideal for collaboration, version control, and automation workflows AWS CodeBuild Fully managed build service for compiling, testing, and packaging code Supports custom buildspec configurations and parallel builds Enables automated testing pipelines for CI/CD workflows AWS CodeDeploy Automates application deployments across EC2, Lambda, and on-premises Supports Blue/Green, Rolling, and Canary deployment strategies Reduces downtime and deployment risks with traffic-shifting controls AWS CodePipeline End-to-end CI/CD orchestration with integration across AWS DevOps tools Automates build, test, and deploy stages with full version tracking Ideal for continuous delivery, multi-environment pipelines, and workflow automation AWS CloudFormation Declarative templates for provisioning and managing AWS resources Stack management with drift detection and rollback capabilities Ensures reproducible, consistent infrastructure automation AWS CDK (Cloud Development Kit) Infrastructure as Code using familiar programming languages High-level constructs and reusable patterns for rapid development Integrates seamlessly with CloudFormation for deployment Container Services on AWS Docker fundamentals for microservices and containerization Amazon ECR for image storage, scanning, and lifecycle policies Amazon ECS \u0026amp; EKS for scalable, orchestrated container deployments AWS App Runner for simplified container-based application hosting Monitoring \u0026amp; Observability Amazon CloudWatch: metrics, logs, alarms, and dashboards AWS X-Ray: distributed tracing and performance analysis Enables full-stack monitoring and operational visibility DevOps Best Practices \u0026amp; Case Studies Deployment strategies: feature flags, A/B testing, and canary releases Automated testing integration within CI/CD pipelines Incident management workflows and effective postmortems Case studies: Startup and enterprise DevOps transformations Event Experience Attending the ​DevOps on AWS I gained more knowledge about Devops and other services in AWS, I can know how they work and the benefits that could help me improve my project The event help me know more about devops and try new services in AWS.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "AWS for SAP Automating SAP HANA DB HA Patch using SSM and nZDT by Guilherme Sesterheim and Mohit Biyani | on 22 AUG 2025 | in Amazon EC2, SAP on AWS, Technical How-to | Permalink Share\nIntroduction Keeping your SAP HANA database up to date with the latest patches is crucial for maintaining security, performance, and reliability. However, traditional database patching often requires significant downtime, impacting business operations. While previous AWS guidance covered various automation approaches, this post introduces a new solution that achieves near-zero downtime for High Availability SAP HANA databases using native AWS services.\nUsing AWS Systems Manager and AWS CloudFormation, we’ll show you how to automate SAP HANA database patching for both Red Hat Enterprise Linux (RHEL) and SUSE Linux Enterprise Server (SLES) environments.\nBenefits of this approach Using this AWS-native tooling approach improves SAP HANA database maintenance by integrating critical operational and security features into a unified solution. AWS Systems Manager serves as your central command center, providing real-time monitoring, detailed logging, and automated health validations throughout the update process. The solution intelligently coordinates updates between primary and secondary nodes while maintaining rollback capabilities for operational assurance. By eliminating the need for third-party tools, you not only reduce licensing costs but also benefit from native AWS service integration, including encrypted communications and comprehensive audit trails. This consolidated approach, managed through a single AWS Systems Manager console, delivers enterprise-scale database maintenance with built-in security and compliance controls.\nPre-requisites The following pre-requisites must be met prior to using this code for updating your HANA Database (HDB) in HA setup:\nA pre-configured SAP HANA 2.0+ database environment running in high availability mode on your Amazon EC2 instances. While we won’t cover the initial setup in this blog, we encourage the use of AWS Launch Wizard to automate the deployment and configuration of your SAP workloads such as SAP HANA, or leverage the SAP on AWS documentation if you need assistance with the configuration. Verify that your SAP HANA database EC2 instances are managed by AWS Systems Manager. This is essential as the automation solution leverages Systems Manager’s capabilities for seamless management and operations. If you’re leveraging a central or shared services account for automation (recommended AWS best practice), ensure you have the appropriate cross-account permissions configured before proceeding, refer the link for more information. AWS Systems Manager automation syncs Amazon S3 media files to the /tmp directory on your EC2 instance. Before running the automation, make sure you have enough storage space available in this temporary directory. The amount of space needed depends on the size of the files for the update you’re performing. Add a tag to your SAP HANA database EC2 instances with the key-value pair “Hostname: ”. You’ll need this hostname value when you run the solution in a later step. Architecture Diagram The architecture diagram (in Image 1) illustrates the solution using AWS Systems Manager Automation Documents, an Amazon S3 bucket containing SAP HANA patch installation media, and essential parameters stored in AWS Secrets Manager. The SAP HANA workloads run on Amazon EC2 instances within your AWS account.\nImage 1 – HANA DB HA Cluster [Diagram illustrating the solution architecture]\nPreparing for the execution Setup a user account in your SAP HANA SYSTEMDB with sufficient privileges to perform database updates. This user will be referenced in the automation process, so it’s essential to verify the account has all necessary authorizations before proceeding with the upgrade. We strongly recommend following the principle of least privilege when configuring your SAP HANA database user permissions. For more details see, Create a Lesser-Privileged Database User for Update.\nMake sure your SAP HANA database instances have the necessary permissions to access the Amazon S3 bucket containing your SAP HANA patch installation media. For detailed instructions on creating and attaching IAM policies to EC2 instances, see working with IAM Roles in the AWS IAM User Guide. Snippet 1 is a sample policy which demonstrates how to grant a specific IAM role within your account the necessary permissions to download files from your S3 bucket. Make sure you change the bucket and role names (highlighted in yellow) according to your environment.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AddPerm\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::{account_id}:role/service-role/{ec2_role}\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::{bucket_name}/*\u0026#34;, \u0026#34;arn:aws:s3:::{bucket_name}\u0026#34; ] } ] } Snippet 1 – S3 Bucket Policy\nThe automation relies on AWS Secrets Manager to retrieve the SAP HANA database credentials. AWS Secrets Manager allows you to share secrets across same/different AWS accounts. This functionality lets you centralize secret management in a single location. For additional details refer to How do I share AWS Secrets Manager secrets between AWS accounts. Make sure you have created the required secrets (sapadm user password and SYSTEM user password) in AWS Secrets Manager and configured appropriate permissions for your target AWS account to access these secrets.\nTo enable secure cross-account access to sensitive information, the solution uses AWS Secrets Manager with AWS KMS encryption. The encrypted secrets are protected by a KMS key that must be accessible to all participating AWS accounts. For detailed guidance on configuring cross-account secrets access, please refer to the documentation.\nHow to run the code Follow the steps below to use the code contained in this GitHub repository to get your HANA Databases updated.\nCreate the secrets required on AWS Secrets Manager on the same account you have your HANA databases. For your reference, we’ve included a sample secret for how to configure on image 2. This example demonstrates the expected format and key-value pairs needed for the automation to work correctly. Please ensure your secrets follow a similar structure while using your actual credentials.\nImage 2 – Example secret for DB credentials\nEstablish an S3 bucket that will serve as the storage location for your update files.\nUse the CloudFormation creating a stack guide to deploy the solution. In the repository, under folder cloudformation, refer below 2 files:\nhana_db_patch_ha_rhel – to be used for databases configured for HA in a RHEL system. This will achieve the nZDT concept explained in the Introduction. hana_db_patch_ha_suse – to be used for databases configured for HA in a SUSE system. This will achieve the nZDT concept explained in the Introduction. Go to Systems Manager \u0026gt; Documents \u0026gt; select tab “Owned by me” \u0026gt; search for “patch” and open your applicable doc:\nImage 3 – SSM Documents available to use [Screenshot of SSM Documents console]\nSelect “Execute automation” on the top right corner:\nImage 4 – SSM Execution Steps [Screenshot of the \u0026ldquo;Execute automation\u0026rdquo; button]\nFill in the required input parameters:\nImage 5 – SSM Documents Input Parameters [Screenshot of the SSM document input parameters]\nScroll down and select “Execute”.\nPost successful completion you see a completion message as Image 6.\nImage 6 – SSM Automation Output [Screenshot of a successful automation completion message]\nFlow of Execution Below you find a flow chart with all the steps, and their details, performed by this automation.\nFlow Chart 1 – SSM Execution Sequence [Diagram showing the sequential flow of automation steps]\nThe steps shown in the ‘Flow Chart 1’ execute sequentially. If any step fails, the entire process stops immediately.\nIf you encounter an error, please consult the Troubleshooting section of this blog to diagnose and resolve the issue before continuing.\nTroubleshoot To monitor \u0026amp; help diagnose automation issues, AWS Systems Manager maintains detailed execution logs in both EC2 instances and CloudWatch Logs. These logs capture the step-by-step progress of your automation.\nMonitoring Automation Status: To check the status of your Systems Manager automations:\nOpen the AWS Systems Manager console. In the left navigation pane, choose Automation Choose Configure preferences \u0026gt; Executions View your automation statuses in the Automation executions section Reviewing Execution Details: The AWS Management Console lets you examine each automation execution in detail. You can:\nNavigate through individual automation steps Review the results of each step Identify any failures that occurred during the automation process Troubleshooting with Logs: There are two ways to access automation logs:\nEC2 Instance Logs Path: /var/lib/amazon/ssm/{instance-id}/document/orchestration/{automation_step_execution_id}/awsrunShellScript/0.awsrunShellScript – contains detailed execution logs from the EC2 instance Path: /tmp/hana/patch – contains the files used for the patch procedure Path: /tmp/update – contains the credentials for running the update command CloudWatch Logs Integration You can configure Systems Manager to send automation outputs to Amazon CloudWatch Logs For setup instructions, see [Configuring Amazon CloudWatch Logs for Run Command] Cost Consideration AWS Systems Manager Automation used in this HANA DB patching solution follows a pay-as-you-go model. You’re charged based on the number and duration of automation steps executed, with a generous free tier that includes:\n100,000 automation steps per month, 5,000 seconds of automation execution time per month If you’re using AWS Organizations, this free tier usage is shared across all accounts in your Consolidated Billing family. If you are running other workloads on the same account that are already using your entire free-tier, the costs associated with running this tool will stay below USD $10.\nFor detailed information about cost calculation, please refer AWS Systems Manger Pricing.\nConclusion As demonstrated in this blog post, automating the patching updates for HANA DBs can be easily achieved using AWS native tools. When you are implementing this in your environment, make sure to run first in a non-productive account before going to real business, as some minor OS versions may cause the solution to behave differently from environment to environment.\nBy using this solution you can standardize how the update process happens across different SAP landscapes and environments and have a single source of truth for the process.\nAre you interested in learning more or maybe you would like a better understanding of how you can extend this solution for your project?\nFor more information, contact us at sap-on-aws@amazon.com.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Goals (Sep 22 – Sep 28, 2025) Explore AWS Lambda and serverless computing concepts. Learn how to create Lambda functions and integrate with other AWS services. Understand API Gateway and how to expose Lambda via REST APIs. Practice monitoring Lambda performance and logs using CloudWatch. Activities \u0026amp; Tasks Day Description Start End References Monday, Sep 22 Studied serverless architecture and use cases for AWS Lambda. 22/09/2025 22/09/2025 AWS Lambda Docs Tuesday, Sep 23 Created first Lambda function in Python, tested basic execution, explored console and CLI options. 23/09/2025 23/09/2025 Lambda Getting Started Wednesday, Sep 24 Integrated Lambda with S3 events to trigger execution on object upload. 24/09/2025 24/09/2025 S3 Event Triggers Thursday, Sep 25 Explored API Gateway: created REST API, connected it to Lambda, tested endpoints. 25/09/2025 25/09/2025 API Gateway Docs Friday, Sep 26 Learned to monitor Lambda using CloudWatch logs, set alarms for errors and execution time. 26/09/2025 26/09/2025 CloudWatch Docs Achievements Understood serverless concepts and AWS Lambda workflow. Successfully created and tested Lambda functions triggered by S3 events. Exposed Lambda functions via API Gateway REST endpoints. Practiced monitoring Lambda logs and setting CloudWatch alarms for proactive alerts. Reflection Week 3 provided practical experience in serverless development. Combining Lambda with S3 and API Gateway gave insight into building event-driven, scalable applications without managing servers. Monitoring with CloudWatch ensures reliability in production environments.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - AWS joins the DocumentDB project to build interoperable, open source document database technology In this blog, AWS explains its decision to join the open source DocumentDB project under the Linux Foundation to promote interoperability and innovation in document databases. Amazon DocumentDB, a fully managed MongoDB-compatible database built on PostgreSQL, is widely used by global customers. By contributing to the project, AWS aims to enhance features, performance, and API compatibility while integrating improvements back into Amazon DocumentDB, demonstrating its commitment to open source and encouraging community involvement.\nBlog 2 - Unlock the power of Amazon OpenSearch Service: Your learning guide for search, analytics, and generative AI solutions In this blog, AWS provides a guide to unlocking the full potential of Amazon OpenSearch Service for search, analytics, and generative AI applications. It highlights AWS Skill Builder courses covering fundamentals, AI-driven search, Elasticsearch migration, lexical search, observability, vector databases, and RAG workflows. Hands-on AWS Workshops offer practical experience in search, analytics, AI, log management, and observability. The blog also outlines relevant AWS certifications for validating expertise in OpenSearch, including Data Engineer, Solutions Architect, Machine Learning, and DevOps roles. Engaging with these resources helps professionals stay current with OpenSearch features, best practices, and evolving technologies.\nBlog 3 - Automating SAP HANA DB HA Patch using SSM and nZDT In this blog, AWS explains how to automate SAP HANA database patching for high availability (HA) environments using native AWS tools like Systems Manager and CloudFormation to achieve near-zero downtime. The solution supports both RHEL and SUSE Linux environments, centralizes management, ensures security and compliance, coordinates updates across nodes, and integrates rollback capabilities. Pre-requisites include a configured SAP HANA HA setup, EC2 instances managed by Systems Manager, S3 access for patch files, and Secrets Manager for credentials. The blog details execution steps, monitoring, troubleshooting, and cost considerations, demonstrating how AWS-native automation standardizes and simplifies SAP HANA patch management across environments.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/5-workshop/5.3-dynamodb-s3/",
	"title": "About S3 &amp; DynamoDB",
	"tags": [],
	"description": "",
	"content": "Database \u0026amp; Storage Summary In the game project, we use AWS DynamoDB and AWS S3 to store player data, progress, scores, and avatars.\nDynamoDB – Main Database DynamoDB is used to store:\nPlayer profile information Player progress Player scores across different game areas Created Tables UserProfiles\nPartition Key (PK): userId UserProgress\nPartition Key (PK): userId Scores\nPartition Key (PK): gameArea Sort Key (SK): score DynamoDB Streams Enabled with Stream type: NEW IMAGE DevOps provides the Stream ARN to the backend so Lambda can consume events. S3 – Store Player Avatars S3 Bucket Setup Bucket name: game-avatars Enabled CORS: Allowed methods: PUT, POST, GET Allowed origin: * Note Uploads should only be performed using presigned URLs generated by the backend. Summary You have created:\n3 DynamoDB tables 1 S3 bucket for storing player avatars These resources will be used by the backend to securely manage and store player-related data.\nContent Create DynamoDB Create S3 Bucket "
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/5-workshop/5.4-api-gateway-lambda/5.4.3-lambda/",
	"title": "Lambda",
	"tags": [],
	"description": "",
	"content": "Lambda Functions Two types:\nStandard Lambda (ZIP) Used for: score, leaderboard, money, progress, unlock, task. Create one Lambda per API route. Lambda container Used for avatar AI (OpenCV + MediaPipe). Suggested name: AvatarProcessingLambda Creation: Lambda → Create Function → Container Image → choose image from ECR. DevOps only creates the function; backend implements the code. Summary You have created:\nREST API for the backend WebSocket API for real-time leaderboard Lambda functions (ZIP + Container) All these details are provided to the backend for connection and deployment.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/4-eventparticipated/4.4-event4/",
	"title": "AWS Well-Architected Security Pillar",
	"tags": [],
	"description": "",
	"content": "Summary Report: AWS Well-Architected Security Pillar (AWS Cloud Mastery Series #3) Event Objectives Dive deeper into IAM, IAM Identity Center, SSO, SSP Learn about CloudTrail, GuardDuty, Security Hub, and automation with EventBridge Protect infrastructure: VPC segmentation, private vs public placement Explore Data Protection and Incident Response Speakers Le Vu Xuan Anh: AWS Cloud Club Captain HCMUTE First Cloud AI Journey Tran Duc Anh: AWS Cloud Club Captain SGU First Cloud AI Journey Tran Doan Cong Ly: AWS Cloud Club Captain PTIT First Cloud AI Journey Danh Hoang Hieu Nghi: AWS Cloud Club Captain HUFLIT First Cloud AI Journey Nguyen Tuan Thinh Cloud: Engineer Trainee First Cloud AI Journey Nguyen Do Thanh Dat: Cloud Engineer Trainee First Cloud AI Journey Mendel Grabski (Long): Ex Head of Security and DevOps Cloud Security Solution Architect Tinh Truong: AWS Community Builder Platform Engineer at TymerX Key Highlights Identity \u0026amp; Access Management Understand the core concepts of IAM: Users, Roles, Policies, and why long-term credentials should be avoided Explore IAM Identity Center (SSO) with permission assignments Practice security best-practices: MFA, credential rotation, IAM Access Analyzer Detection Learn how CloudTrail enables full auditing across multiple accounts Explore threat-detection services: GuardDuty and Security Hub Understand multi-layer logging: VPC Flow Logs, ALB logs, S3 access logs Build alerting and automation workflows using EventBridge Infrastructure Protection KMS: key policies, grants, rotation Data encryption at-rest \u0026amp; in-transit: S3, EBS, RDS, DynamoDB Secrets Manager \u0026amp; Parameter Store — rotation patterns Data classification \u0026amp; access guardrails Incident Response Incident response lifecycle on AWS Handling compromised IAM keys Detecting S3 public exposure Detecting malware on EC2 Snapshot, isolation, evidence collection Automated responses using Lambda/Step Functions Event Experience Attending the AWS Well-Architected Security Pillar event not only helped me learn new services, but also meet people from other universities, learn more technologies, and interact with foreigners working at AWS who shared how they work. The event helped me better understand security, learn about incidents and solutions, and motivated me to join the Cloud Club for more hands-on experiences.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Goals (Sep 29 – Oct 5, 2025) Learn about DynamoDB and NoSQL database concepts. Practice creating tables, inserting data, and querying with different key types. Understand primary keys, sort keys, and indexes in DynamoDB. Explore integration of DynamoDB with Lambda for serverless applications. Activities \u0026amp; Tasks Day Description Start End References Monday, Sep 29 Studied NoSQL principles and use cases, overview of DynamoDB features. 29/09/2025 29/09/2025 DynamoDB Docs Tuesday, Sep 30 Created first DynamoDB table, explored partition keys and sort keys. 30/09/2025 30/09/2025 DynamoDB Getting Started Wednesday, Oct 1 Inserted and updated sample data, practiced queries using partition key and sort key. 01/10/2025 01/10/2025 DynamoDB Queries Thursday, Oct 2 Explored secondary indexes and how to use them for efficient queries. 02/10/2025 02/10/2025 DynamoDB Indexes Friday, Oct 3 Integrated DynamoDB with Lambda: triggered Lambda functions on table updates. 03/10/2025 03/10/2025 Lambda + DynamoDB Achievements Learned key concepts of DynamoDB and NoSQL data modeling. Successfully created tables and performed CRUD operations. Applied partition keys, sort keys, and secondary indexes in queries. Integrated DynamoDB with Lambda for event-driven serverless workflows. Reflection Week 4 provided practical experience in building scalable NoSQL applications on AWS. DynamoDB’s integration with Lambda helped me understand event-driven architecture and reinforced my knowledge of serverless database interactions.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in total 4 events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments in my life.\nEvent 1 Event Name: Kick-off AWS FCJ Workforce -FPTU OJT FALL 2025\nDate \u0026amp; Time: 09:00, September 08, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering (AWS Cloud Mastery Series #1)\nDate \u0026amp; Time: 09:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: ​DevOps on AWS (AWS Cloud Mastery Series #2)\nDate \u0026amp; Time: 09:00, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: AWS Well-Architected Security Pillar (AWS Cloud Mastery Series #3)\nDate \u0026amp; Time: 09:00, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/5-workshop/5.4-api-gateway-lambda/",
	"title": "API &amp; Lambda Overview",
	"tags": [],
	"description": "",
	"content": "API \u0026amp; Lambda Overview This section summarizes all API and compute services used in the game backend.\nThe system includes:\nREST API Gateway – Main API for gameplay actions WebSocket API Gateway – Real-time leaderboard communication AWS Lambda Functions – Backend compute for all game logic and avatar processing Below is a combined overview of how these services operate together.\nREST API Gateway – Backend API Layer The REST API connects the frontend (Unity/Web) to the backend Lambda functions.\nMain Endpoints POST /score GET /leaderboard GET /leaderboard/global POST /progress POST /unlock POST /task/complete POST /money/add POST /shop/buy POST /avatar/presign POST /avatar/update POST /avatar/process (proxy to the avatar container Lambda) Configuration Enable CORS for all routes Use JWT Authorizer connected to Amazon Cognito Each route is mapped to its own Lambda handler REST API ensures secure and structured communication for all core game features.\nWebSocket API – Real-Time Leaderboard The WebSocket API handles live leaderboard updates.\nRoutes $connect → store connectionId $disconnect → remove connectionId broadcast → push updated leaderboard to all players Configuration Lambda functions read active connectionIds from DynamoDB Ideal for timely score updates without polling This service allows the game to display real-time ranking updates.\nLambda Functions – Game Logic Execution Two Lambda types are used:\n1. Standard Lambda (ZIP) Used for standard game logic:\nscoring leaderboard player money progress saving unlocking features task system Each API route has a dedicated Lambda function to keep logic clean and modular.\n2. Container Lambda Used for heavy AI processing:\nAvatar generation (OpenCV + MediaPipe) Function name suggestion: AvatarProcessingLambda Deployed from an ECR container image.\nDevOps prepares the function, and backend developers implement the code.\nSummary You have successfully set up:\nA REST API for game logic communication A WebSocket API for real-time leaderboard Multiple Lambda functions (ZIP + Container) for all backend processing These services form the core backend infrastructure for the game, enabling secure API access, real-time updates, and AI-powered avatar processing.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Goals (Oct 6 – Oct 12, 2025) Dive deeper into AWS S3: advanced features and security. Understand bucket policies, ACLs, and encryption options. Practice versioning, replication, and lifecycle rules. Learn to integrate S3 with Lambda and other AWS services for automation. Activities \u0026amp; Tasks Day Description Start End References Monday, Oct 6 Explored S3 bucket policies and ACLs to control access at granular level. 06/10/2025 06/10/2025 S3 Security Docs Tuesday, Oct 7 Implemented server-side encryption (SSE) with AWS-managed keys for sensitive data. 07/10/2025 07/10/2025 S3 Encryption Wednesday, Oct 8 Enabled versioning on buckets, uploaded multiple versions of objects, practiced restoring old versions. 08/10/2025 08/10/2025 S3 Versioning Guide Thursday, Oct 9 Configured lifecycle rules for automatic transition to IA/Glacier and automatic deletion. 09/10/2025 09/10/2025 S3 Lifecycle Docs Friday, Oct 10 Connected S3 events to Lambda for automation, tested triggers on object upload and deletion. 10/10/2025 10/10/2025 Lambda + S3 Events Achievements Mastered S3 security features including policies, ACLs, and encryption. Practiced versioning and lifecycle management to optimize storage. Automated workflows by connecting S3 events to Lambda functions. Gained confidence in designing secure and efficient S3-based solutions. Reflection Week 5 enhanced my understanding of advanced S3 features and their practical applications. Integrating S3 with Lambda demonstrated the power of automation in cloud storage workflows, while lifecycle and versioning taught cost-effective data management.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/5-workshop/5.5-ci-cd/",
	"title": "CI/CD Deployment for Backend",
	"tags": [],
	"description": "",
	"content": "Objective In this section, you will configure an automated pipeline to deploy the backend, including both regular Lambda and Lambda container (AvatarProcessingLambda). CI/CD ensures correct IAM permissions and high availability of services.\n1. Prerequisites Backend GitHub repository ECR repository for Lambda container (AvatarProcessingLambda) IAM Role for CodeBuild with permissions: ECR push/pull, Lambda update 2. Create CodePipeline Go to AWS CodePipeline → Create Pipeline Name the pipeline (e.g., GameBackendPipeline) Select Source: GitHub repo Select Build: CodeBuild 3. Configure CodeBuild Environment: Managed image + Docker Sample buildspec.yml: version: 0.2 phases: pre_build: commands: - echo Logging in to Amazon ECR... - aws ecr get-login-password --region ap-southeast-2 | docker login --username AWS --password-stdin 254670366571.dkr.ecr.ap-southeast-2.amazonaws.com build: commands: - echo Build Docker image... - docker build -t avatar-processing . - docker tag avatar-processing:latest 254670366571.dkr.ecr.ap-southeast-2.amazonaws.com/avatar-processing:latest post_build: commands: - echo Push Docker image to ECR... - docker push 254670366571.dkr.ecr.ap-southeast-2.amazonaws.com/avatar-processing:latest - echo Deploy to Lambda... - aws lambda update-function-code --function-name AvatarProcessingLambda --image-uri 254670366571.dkr.ecr.ap-southeast-2.amazonaws.com/avatar-processing:latest "
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Workshop Overview — Game Backend Project General Objectives This workshop walks you step-by-step through deploying a complete backend for a Unity/Web game on AWS. After completing it, you will have:\nUser authentication via Cognito (username/password + Google OAuth). A NoSQL database using DynamoDB for profiles, progress, and scores. Player avatar storage on S3 with presigned URL uploads. Lambda functions (ZIP) for lightweight logic and a Container Lambda (OpenCV/MediaPipe) for heavy image/AI processing. API Gateway (REST) for gameplay routes and API Gateway (WebSocket) for real‑time leaderboard. CI/CD pipeline (CodePipeline + CodeBuild) for automatic container build, ECR deployment, and Lambda updates. IAM configuration, CloudWatch monitoring/logging, and optional WAF. Workshop Structure Cognito — Set up authentication + JWT authorizer. DynamoDB — Create tables: UserProfiles, UserProgress, Scores. S3 — Create game-avatars bucket, enable CORS, use presigned URLs. ECR — Create repository for Container Lambda images. Lambda — Implement ZIP functions + container-based Lambda for image processing. API Gateway (REST) — Define routes and integrate with Lambda. API Gateway (WebSocket) — Manage realtime leaderboard communication. CI/CD — Set up CodePipeline + CodeBuild to automate builds and deployments. IAM — Create proper roles for Lambda, CodeBuild, and CodePipeline. Logging \u0026amp; Monitoring — Configure CloudWatch logs and alerts. WAF — (Optional) Protect API from malicious requests. Technical Details DynamoDB Main tables: UserProfiles (PK: userId) UserProgress (PK: userId) Scores (PK: gameArea, SK: score) Streams enabled with NEW_IMAGE for realtime event processing. S3 Bucket: game-avatars CORS: allow PUT, POST, GET (production should be more restrictive). Uploads handled via presigned URLs generated by backend. API \u0026amp; Lambda Common REST endpoints: POST /score, GET /leaderboard, GET /leaderboard/global POST /progress, POST /unlock, POST /task/complete POST /money/add, POST /shop/buy POST /avatar/presign, POST /avatar/update, POST /avatar/process WebSocket routes: $connect, $disconnect, broadcast Lambda Types: ZIP Lambdas for scoring, shop, progress, leaderboard. Container Lambda AvatarProcessingLambda using OpenCV + MediaPipe. CI/CD (CodePipeline + CodeBuild) Source: GitHub repository Build: CodeBuild (Docker enabled) → build image → push to ECR → update Lambda image. Provided buildspec.yml logs in to ECR, builds the image, pushes it, and updates Lambda. IAM Required permissions include Lambda, API Gateway, Cognito, DynamoDB, S3, CodeBuild, CodePipeline, CloudWatch, SNS, and iam:PassRole. Monitoring CloudWatch: log groups for Lambda, alarms for billing and errors. Workshop Outcomes Full backend system for a game: REST API, realtime WebSocket, database, avatar storage, and automated deployment pipeline. Configuration notes (ARNs, endpoints, presigned URL formats) for frontend/backend developers. Content Workshop overview Prerequiste Creating DynamoDB and S3 Bucket Creating API Gateway \u0026amp; Lambda More about CI/CD Clean up "
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/6-self-evaluation/",
	"title": "Self-Evaluation",
	"tags": [],
	"description": "",
	"content": "During my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to gain hands-on experience, apply the knowledge learned at school, and develop professional skills.\nI participated in [brief description of project or main tasks], which helped me enhance my [list skills: programming, analysis, reporting, communication, etc.].\nRegarding work habits, I strive to complete tasks effectively, follow company rules, and actively communicate with colleagues to improve work outcomes.\nBelow is my self-assessment based on key criteria:\nNo. Criterion Description Excellent Good Average 1 Professional Knowledge \u0026amp; Skills Understanding the field, applying knowledge, using tools, quality of work ✅ ☐ ☐ 2 Learning Ability Quickly acquiring new knowledge and skills ☐ ✅ ☐ 3 Proactivity Taking initiative and accepting tasks without being told ✅ ☐ ☐ 4 Responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Growth Mindset Willingness to receive feedback and improve ☐ ✅ ☐ 7 Communication Clearly presenting ideas and reporting work ☐ ✅ ☐ 8 Teamwork Collaborating effectively with colleagues and contributing to the team ✅ ☐ ☐ 9 Professional Conduct Respecting colleagues, clients, and the work environment ✅ ☐ ☐ 10 Problem Solving Identifying problems, proposing solutions, being creative ☐ ✅ ☐ 11 Contribution to Project/Org Work efficiency, innovative ideas, recognition from team ✅ ☐ ☐ 12 Overall Performance General assessment of the entire internship experience ✅ ☐ ☐ Areas for Improvement Strengthen discipline and adherence to company or organizational rules. Improve problem-solving skills and propose effective solutions. Enhance communication skills for daily interactions and work situations. "
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Goals (Oct 13 – Oct 19, 2025) Learn about AWS CloudFormation for infrastructure as code (IaC). Practice creating and managing stacks using templates. Understand resource dependencies, parameters, and outputs in CloudFormation. Explore best practices for versioning and organizing templates. Activities \u0026amp; Tasks Day Description Start End References Monday, Oct 13 Studied CloudFormation concepts, benefits of IaC, and basic template structure. 13/10/2025 13/10/2025 CloudFormation Docs Tuesday, Oct 14 Created first CloudFormation stack with S3 bucket and IAM role. 14/10/2025 14/10/2025 CloudFormation Tutorial Wednesday, Oct 15 Learned to use parameters and outputs to make templates reusable and modular. 15/10/2025 15/10/2025 CloudFormation Parameters Thursday, Oct 16 Explored resource dependencies and how CloudFormation handles stack creation and deletion. 16/10/2025 16/10/2025 Stack Management Friday, Oct 17 Practiced updating stacks safely and managing template versions in S3. 17/10/2025 17/10/2025 CloudFormation Best Practices Achievements Gained solid understanding of CloudFormation templates and stack management. Successfully created reusable, modular templates with parameters and outputs. Learned to handle resource dependencies and safely update stacks. Familiar with best practices for template versioning and organization. Reflection Week 6 enhanced my ability to manage AWS infrastructure programmatically. Using CloudFormation improved efficiency and repeatability, which is crucial for large-scale cloud projects. Understanding dependencies and stack updates gave me confidence in deploying complex environments safely.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "5.6 Cleaning up the Workshop Environment After completing the workshop, clean up the AWS environment to avoid unnecessary charges.\nDelete Lambda Functions Go to AWS Lambda Console. Select all Lambda Functions created for the workshop: AvatarProcessingLambda BroadcastHandler ConnectHandler DisconnectHandler LeaderboardFunction MoneyFunction ProgressFunction ScoreFunction TaskFunction UnlockFunction Click Actions → Delete and confirm. Delete API Gateway Go to API Gateway Console. Select REST API and WebSocket API created for the workshop. Click Actions → Delete API and confirm. Delete S3 Buckets Go to S3 Console. Delete all buckets created: game-avatars Workshop-specific buckets (bucket-1, bucket-2, …) Make sure to delete all objects inside each bucket before deleting the bucket. Delete DynamoDB Tables Go to DynamoDB Console. Delete the following tables: UserProfiles UserProgress Scores Confirm deletion of each table. Delete IAM Roles Go to IAM Console → Roles. Delete the roles created for the workshop: Lambda Execution Role CodeBuild Role Ensure no policies are attached before deleting roles. Delete ECR Repositories Go to ECR Console. Delete the avatar-processing repository. Confirm deletion of all images in the repository. Delete CodePipeline / CodeBuild Go to CodePipeline Console. Delete the workshop pipeline. Delete the corresponding CodeBuild project. Check Costs Go to AWS Billing Console → Cost Explorer. Make sure there are no running resources incurring costs. Delete any remaining resources if necessary. Summary The workshop environment has been fully cleaned. No Lambda, API Gateway, S3, DynamoDB, IAM Roles, ECR, or pipelines remain. Costs from the workshop should now be zero. "
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/7-feedback/",
	"title": "Internship Reflections",
	"tags": [],
	"description": "",
	"content": " In this section, I reflect on my internship experience at AWS, what I have learned, and suggestions for improving the program.\nOverall Evaluation 1. Work Environment\nThe work environment at AWS is welcoming and inclusive. Colleagues are always willing to provide guidance and support whenever I encounter challenges. The workspace is well-equipped and conducive to focus. I think it would be even more engaging if there were more team activities or knowledge-sharing sessions across departments.\n2. Mentor and Team Support\nMy mentor are always serious and are always be to help the interns. The admin team facilitated all administrative tasks efficiently, allowing me to focus on learning and project work.\n3. Alignment with Academic Knowledge\nThe tasks assigned during my internship connected well with what I studied at university, while also exposing me to technologies and tools I had never used before. For example, working with Amazon OpenSearch helped me understand real-world search and analytics solutions.\n4. Learning and Skill Development\nThroughout the internship, I enhanced my programming, data analysis, and log analytics skills. I also learned professional communication and teamwork. The mentor shared practical insights that helped me better plan my career in cloud computing.\n5. Company Culture and Team Spirit\nAWS fosters a culture of respect, collaboration, and learning. Team members support each other during tight deadlines, and the environment encourages sharing ideas openly. Even as an intern, I felt like a valued part of the team.\n6. Internship Policies and Benefits\nThe program provides a stipend, flexible working hours, and access to internal training resources. These benefits made it easier for me to focus on gaining meaningful experience.\nFeedback Questions What was the most rewarding aspect of your internship? Gaining hands-on experience with AWS services and having supportive mentors. What could the company improve for future interns? Organize more mini-projects or workshops to allow interns to apply skills early. Would you recommend this internship to a friend? Why or why not? Yes, the learning environment and practical exposure are highly valuable. Suggestions \u0026amp; Expectations Increase cross-team knowledge-sharing sessions to broaden perspectives. Provide opportunities to participate in end-to-end projects, from design to deployment. Additional comments: I truly appreciate the dynamic environment and support from mentors and team members. "
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Goals (Oct 20 – Oct 26, 2025) Explore AWS CloudWatch in depth for monitoring and logging. Learn to create custom metrics, dashboards, and alarms. Practice analyzing logs and setting automated notifications. Integrate CloudWatch with Lambda and other services for real-time monitoring. Activities \u0026amp; Tasks Day Description Start End References Monday, Oct 20 Reviewed CloudWatch core concepts, metrics, logs, and dashboards. 20/10/2025 20/10/2025 CloudWatch Docs Tuesday, Oct 21 Created custom metrics for EC2 instances and monitored CPU/memory usage. 21/10/2025 21/10/2025 Custom Metrics Wednesday, Oct 22 Built dashboards to visualize metrics and set up alarms for thresholds. 22/10/2025 22/10/2025 CloudWatch Dashboards Thursday, Oct 23 Practiced analyzing CloudWatch logs for Lambda functions and S3 events. 23/10/2025 23/10/2025 CloudWatch Logs Friday, Oct 24 Integrated alarms with SNS to receive notifications for critical events. 24/10/2025 24/10/2025 SNS Integration Achievements Mastered creating custom metrics and dashboards for EC2 and Lambda. Learned to analyze logs and detect anomalies proactively. Set up alarms and integrated with SNS for notifications. Gained practical skills to monitor AWS infrastructure efficiently. Reflection Week 7 strengthened my understanding of monitoring AWS services. Using CloudWatch dashboards and alarms provided real-time insights and allowed me to respond quickly to operational issues. Integration with SNS demonstrated effective automated alerting for cloud environments.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Goals (Oct 27 – Nov 2, 2025) Explore AWS Elastic Beanstalk for deploying web applications. Learn to deploy, update, and monitor applications. Understand environment configurations, scaling options, and health monitoring. Practice integrating Beanstalk with RDS and S3 for a full-stack application. Activities \u0026amp; Tasks Day Description Start End References Monday, Oct 27 Studied Elastic Beanstalk concepts and benefits of PaaS for application deployment. 27/10/2025 27/10/2025 Elastic Beanstalk Docs Tuesday, Oct 28 Deployed a sample web application, explored environment options, and monitored initial health. 28/10/2025 28/10/2025 EB Deployment Guide Wednesday, Oct 29 Configured auto-scaling and load balancing for high availability. 29/10/2025 29/10/2025 Scaling in Beanstalk Thursday, Oct 30 Connected Beanstalk application with RDS database and S3 storage for full-stack functionality. 30/10/2025 30/10/2025 EB Integration Friday, Oct 31 Practiced updating application versions, rolling deployments, and rollback strategies. 31/10/2025 31/10/2025 EB Best Practices Achievements Gained hands-on experience deploying applications using Elastic Beanstalk. Configured environment settings, auto-scaling, and load balancing. Successfully integrated RDS and S3 for a full-stack web application. Learned update, rollback, and monitoring strategies for production-ready deployments. Reflection Week 8 improved my understanding of PaaS on AWS. Elastic Beanstalk simplified deployment and management, while integration with RDS and S3 allowed me to build a complete application stack. Practicing updates and rollback enhanced confidence in managing production environments.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Goals (Nov 3 – Nov 9, 2025) Explore AWS VPC and networking fundamentals. Learn to create and configure subnets, route tables, and internet gateways. Understand security groups, NACLs, and VPC peering. Practice deploying EC2 instances within VPCs and testing connectivity. Activities \u0026amp; Tasks Day Description Start End References Monday, Nov 3 Studied VPC concepts, private vs public subnets, and the benefits of isolated networks. 03/11/2025 03/11/2025 VPC Docs Tuesday, Nov 4 Created a custom VPC, defined subnets, and configured route tables and IGW. 04/11/2025 04/11/2025 VPC Tutorial Wednesday, Nov 5 Set up security groups and NACLs for controlling inbound/outbound traffic. 05/11/2025 05/11/2025 VPC Security Thursday, Nov 6 Practiced VPC peering between two VPCs and tested cross-VPC communication. 06/11/2025 06/11/2025 VPC Peering Guide Friday, Nov 7 Launched EC2 instances in different subnets, tested connectivity, and practiced troubleshooting network issues. 07/11/2025 07/11/2025 EC2 Networking Achievements Gained hands-on experience creating and managing VPCs and subnets. Learned to configure route tables, internet gateways, security groups, and NACLs. Practiced VPC peering and cross-VPC communication. Successfully launched EC2 instances and verified network connectivity. Reflection Week 9 strengthened my understanding of AWS networking and VPC design. Building isolated networks, configuring security, and testing connectivity provided practical skills essential for secure cloud architectures.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Goals (Nov 10 – Nov 16, 2025) Explore AWS Lambda and serverless computing in depth. Learn to create, test, and deploy Lambda functions using Python. Understand triggers from S3, CloudWatch, and API Gateway. Practice logging, monitoring, and handling errors in Lambda. Activities \u0026amp; Tasks Day Description Start End References Monday, Nov 10 Reviewed Lambda fundamentals and advantages of serverless architecture. 10/11/2025 10/11/2025 AWS Lambda Docs Tuesday, Nov 11 Created simple Python Lambda functions and tested locally and on AWS console. 11/11/2025 11/11/2025 Lambda Getting Started Wednesday, Nov 12 Configured S3 triggers to invoke Lambda functions on object uploads. 12/11/2025 12/11/2025 Lambda + S3 Thursday, Nov 13 Set up CloudWatch logs and alarms for monitoring Lambda executions and failures. 13/11/2025 13/11/2025 Lambda Monitoring Friday, Nov 14 Connected API Gateway to Lambda functions and tested serverless REST APIs. 14/11/2025 14/11/2025 API Gateway Integration Achievements Gained hands-on experience creating and deploying Python Lambda functions. Learned to use S3 and API Gateway as event sources for Lambda. Configured logging, monitoring, and alarms for error detection. Practiced building serverless APIs and event-driven workflows. Reflection Week 10 deepened my knowledge of serverless architecture on AWS. Lambda allowed me to deploy scalable functions without managing servers, while integration with S3, CloudWatch, and API Gateway provided practical experience in event-driven cloud solutions.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Goals (Nov 17 – Nov 23, 2025) Explore AWS RDS and managed databases. Learn to create and configure RDS instances (MySQL/PostgreSQL). Understand backups, snapshots, scaling, and security best practices. Practice connecting RDS with applications and performing queries. Activities \u0026amp; Tasks Day Description Start End References Monday, Nov 17 Studied RDS concepts, benefits of managed databases, and available engines. 17/11/2025 17/11/2025 RDS Docs Tuesday, Nov 18 Created a MySQL RDS instance, configured security groups, and tested connectivity. 18/11/2025 18/11/2025 RDS Setup Guide Wednesday, Nov 19 Explored automated backups, snapshots, and point-in-time recovery. 19/11/2025 19/11/2025 RDS Backup Thursday, Nov 20 Practiced scaling RDS instances vertically and horizontally, and monitored performance. 20/11/2025 20/11/2025 RDS Scaling Friday, Nov 21 Connected RDS to an application (Python Flask), executed queries, and tested CRUD operations. 21/11/2025 21/11/2025 RDS Application Integration Achievements Successfully created and configured RDS instances with secure access. Learned backup strategies and disaster recovery options. Practiced scaling and monitoring database performance. Integrated RDS with an application and performed queries efficiently. Reflection Week 11 strengthened my understanding of AWS managed databases. Working with RDS improved my ability to deploy reliable, scalable, and secure database solutions. Integration with applications provided practical experience for full-stack cloud development.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Goals (November 24 – 30, 2025) Review and consolidate all AWS services learned in previous weeks. Practice building an end-to-end project integrating multiple AWS services: EC2, S3, RDS, Lambda, CloudWatch. Learn about cost optimization, resource tagging, and applying best practices. Prepare documentation and evaluate skills acquired over 12 weeks. Activities \u0026amp; Tasks Day Activity Start End Reference Materials Monday, 24/11 Review notes and documentation on EC2, S3, RDS, Lambda, VPC, CloudWatch, and CloudFormation. 24/11/2025 24/11/2025 AWS Docs Tuesday, 25/11 Design the architecture of the end-to-end project integrating EC2, S3, and RDS. 25/11/2025 25/11/2025 AWS Architecture Center Wednesday, 26/11 Deploy Lambda functions triggered by S3 and integrate CloudWatch for monitoring. 26/11/2025 26/11/2025 AWS Lambda Docs Thursday, 27/11 Apply tagging strategy, monitor costs using CloudWatch and Budgets, practice security best practices. 27/11/2025 27/11/2025 AWS Cost Management Friday, 28/11 Test the end-to-end project, create documentation, and evaluate lessons learned as well as improvements. 28/11/2025 28/11/2025 Personal notes \u0026amp; AWS docs Achievements Successfully designed and deployed a full-stack AWS project integrating multiple services. Practiced cost optimization and resource tagging for effective management. Reviewed and reinforced knowledge from all previous weeks. Prepared final documentation and gained confidence in managing AWS end-to-end infrastructure. Evaluation Week 12 marks the conclusion of 12 weeks of intensive AWS learning. Consolidating knowledge into a project helped me clearly understand real-world workflows, cost management, and security. I am confident in applying AWS skills to real projects and maintaining scalable, secure cloud environments.\n"
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://halricklee.github.io/OJT_Fall2025/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]